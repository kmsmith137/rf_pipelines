#!/usr/bin/env python

import sys
import argparse


####################################################################################################
#
# Argument parsing, checking


# Got a little carried away here, and wrote an argparse.ArgumentParser 
# subclass, to customize error reporting...

class MyParser(argparse.ArgumentParser):
    def print_usage(self, file=None):
        if file is None:
            file = sys.stdout

        print >>file, 'Usage: rfp-run [-Wnosh] [-w run_name] [-v verbosity] [-t nthreads] file1.json [file2.json file3.json ...]'
        print >>file, 'Each json file should contain either a jsonized pipeline_object, or a \"run-list\" of [suffix, json_filename] pairs'
        print >>file, '    -n: runs the pipeline with no output directory'
        print >>file, '    -o: show stdout during pipeline run (by default, stdout is suppressed, but stderr is shown)'
        print >>file, '    -s: throws exception unless single pipeline run (i.e. no run-lists allowed), infrequently used'
        print >>file, '    -w: runs the pipeline in a directory which is indexed by the web viewer, with explicitly specified run_name'
        print >>file, '    -W: runs the pipeline in a directory which is indexed by the web viewer (run-list only, run_names will be obtained from run-list)'
        print >>file, '    -v: specifies pipeline verbosity (integer, default 2)'
        print >>file, '    -t: number of threads (default 1, note that multiple threads are only useful if at least one json file is a run-list)'
        print >>file, '    -h: show longer help message and exit'


    def error(self, message=None):
        self.print_usage(sys.stderr)

        if message is not None:
            print >>sys.stderr, '\nError:', message

        sys.exit(2)

    
    def print_help(self, file=None):
        self.print_usage(file)

        print >>file
        print >>file, "The rfp-run utility runs a pipeline from the command line.  The pipeline is constructed from a sequence of"
        print >>file, "pipeline_objects which have previously been serialized with jsonize(), and specified on the command line."
        print >>file
        print >>file, 'Alternatively, each command-line json file can be a \"run-list\" which points to a list of pipeline_object json files.'
        print >>file, 'In this case, multiple pipeline runs are performed in batch processing mode.'
        print >>file
        print >>file, "A run-list file is just a json file containing a list of [suffix, json_filename] string pairs.  The run-list 'suffix'"
        print >>file, "is a short string such as 'run1' which is appended to the web viewer run_name, so that we get a unique run_name for"
        print >>file, "each run.  The run-list 'json_filename' is interpreted relative to the directory containing the run-list (not the"
        print >>file, "current working directory).  The run-list file format is intended to be minimal enough that run-lists are easy to"
        print >>file, "make by hand.  There are lots of examples of run-lists in https://github.com/mrafieir/ch_frb_rfi."
        print >>file
        print >>file, "If the -w or -W flag is specified, then the run will be viewable in the web viewer after it completes."
        print >>file, "Exactly one of the -w,-n flags must be specified."
        print >>file
        print >>file, "If multiple threads are specified with -t NTHREADS, then multiple pipeline runs will be performed in parallel.  This only helps"
        print >>file, "if run-lists are being used (so that there is more than one pipeline run to perform).  It often makes sense to set NTHREADS equal"
        print >>file, "to the number of cores in the node."
        print >>file
        print >>file, "By default, the pipeline's stdout is not displayed to the screen, whereas stderr is.  Generally speaking, in rf_pipelines, we"
        print >>file, "try to observe a convention where stderr is used to report warnings and unusual events, and stdout is used to report routine"
        print >>file, "events.  If the -o flag is specified, then both stdout and stderr will be displayed.  Note that for a web viewer run (-w),"
        print >>file, "stdout always gets written to a log file, which can be viewed afterwards in the web viewer."
        print >>file
        print >>file, "The optional environment variable RF_PIPELINE_ATTRS contains additional pipeline attributes (specified as a json object of"
        print >>file, "(key,value) pairs, serialized to a single string).  These attributes will be passed to the _bind() and _start_pipeline()"
        print >>file, "methods of all pipeline_objects, and are also written to the pipeline's json output."


parser = MyParser()

parser.add_argument('json_filenames', nargs='*')
parser.add_argument('-n', action='store_true', help='runs the pipeline with no output directory')
parser.add_argument('-o', action='store_true', help='show stdout during pipeline run (by default, stdout is suppressed, but stderr is shown)')
parser.add_argument('-s', action='store_true', help='throws exception unless single pipeline run (i.e. no run-lists allowed), infrequently used')
parser.add_argument('-w', dest='wv_name', help='runs the pipeline in a directory which is indexed by the web viewer, with explicitly specified run_name')
parser.add_argument('-v', dest='verbosity', type=int, default=2, help='pipeline verbosity (default 2)')
parser.add_argument('-t', dest='nthreads', type=int, default=1, help='number of threads (default 1, note that multiple threads are only useful if at least one json file is a run-list)')
parser.add_argument('-W', action='store_true', help='runs the pipeline in a directory which is indexed by the web viewer (run-list only, run_names will be obtained from run-list)')


args = parser.parse_args()

ocount = 0
if args.n:
    ocount += 1
if args.wv_name is not None:
    ocount += 1
if args.W:
    ocount += 1

if len(sys.argv) == 1:
    parser.error()
if ocount != 1:
    parser.error('exactly one of the flags -n,-w,-W must be specified')
if len(args.json_filenames) == 0:
    parser.error('at least one json filename must be specified')
if args.nthreads < 1:
    parser.error('number of threads (i.e. argument to -t option) must be >= 1')
if args.nthreads > 40:
    parser.error('number of threads (i.e. argument to -t option) must be <= 40')

for j in args.json_filenames:
    if not j.endswith('.json'):
        parser.error("filename '%s' does not end in .json, currently treated as an error" % j)

if args.wv_name is not None:
    if args.wv_name.startswith('_') or args.wv_name.endswith('.json') or (len(args.wv_name) == 0):
        parser.error("invalid web viewer run name '%s" % args.wv_name)
    for s in args.wv_name:
        if s.isspace() or (s == '/'):
            parser.error("invalid web viewer run name '%s" % args.wv_name)


####################################################################################################
#
# Read json files and do some minimal sanity checking.
#
# The result is 'toplevel_filenames', a list of lists of pairs, where
#    outer list index = index of command-line argument
#    inner list index = index within run-list
#    pair = (suffix, json_filename)
#
# If a command-line argument is a pipeline_object (rather than a run-list), then the
# corresponding inner list will have length one, and the 'suffix' part of the pair
# will be an empty string.


import os
import json
import copy
import time
import select
import signal
import traceback
import subprocess
import rf_pipelines


def is_well_formed_run_list(j):
    """Helper function: returns True if j is a list of (suffix, json_filename) pairs."""

    if not isinstance(j, list) or (len(j) == 0):
        return False

    for jj in j:
        return isinstance(jj,list) and (len(jj) == 2) and isinstance(jj[0], basestring) and isinstance(jj[1], basestring) and jj[1].endswith('.json')


toplevel_filenames = [ ]
have_runlists = False


for filename in args.json_filenames:
    j = rf_pipelines.utils.json_read(filename)
    
    if isinstance(j, dict) and j.has_key('class_name'):
        # Case 1: command-line json file is a pipeline_object.
        # Sanity check: verify that dejsonization works.

        rf_pipelines.pipeline_object.from_json(j)
        toplevel_filenames.append([('',filename)])
        continue

    if not is_well_formed_run_list(j):
        print '%s: not a pipeline_object, or a well-formed run-list consisting of (suffix, json_filename) pairs' % filename
        sys.exit(1)

    # Case 2: command-line json file is a run-list.

    inner_filenames = [ ]
    have_runlists = True

    for (suffix, pre_filename2) in j:
        filename2 = os.path.join(os.path.dirname(filename), pre_filename2)

        if (len(suffix) == 0) or (suffix[0] == '_'):
            raise RuntimeError("%s: run-list entry ('%s', '%s') looks suspicious" % (filename, suffix, filename2))
        if not filename2.endswith('.json'):
            raise RuntimeError("%s: run-list entry ('%s', '%s') looks suspicious (second element does not end in '.json')" % (filename, suffix, filename2))
        if args.s:
            raise RuntimeError("%s: run-list cannot be specified with -s" % filename2)

        j2 = rf_pipelines.utils.json_read(filename2)

        if is_well_formed_run_list(j2):
            raise RuntimeError('run-lists cannot contain run-lists (%s contains %s)' % (filename, pre_filename2))
        elif not isinstance(j2,dict) or not j2.has_key('class_name'):
            print '%s: not a pipeline_object, or a well-formed run-list consisting of (suffix, json_filename) pairs' % filename2
            sys.exit(1)

        # Sanity check: verify that dejsonization works.
        rf_pipelines.pipeline_object.from_json(j2)
        inner_filenames.append(('%s' % suffix, filename2))

    toplevel_filenames.append(inner_filenames)


if args.W and not have_runlists:
    raise RuntimeError('if the -W flag is specified, at least one json file must contain a run-list')

total_pipeline_runs = 1

for x in toplevel_filenames:
    total_pipeline_runs *= len(x)
    assert total_pipeline_runs > 0

    if total_pipeline_runs > 1000:
        raise RuntimeError('Total number of pipeline runs exceeds 1000, presumably unintentional')

if total_pipeline_runs < args.nthreads:
    args.nthreads = total_pipeline_runs


####################################################################################################
#
# Generate 'all_runs', a list of (rundir, json_filename_list) pairs.


def generate_runs(base_rundir, base_filename_list, more_filenames):
    """
    Recursive helper function to generate (rundir, json_filename_list) pairs, where 'rundir' can be None.
    (but in this case there is only one (rundir, json_filename_list) pair generated).

    The 'base_rundir' argument can be either:
      - None: no rundir allowed (happens iff -n flag was specified)
      - Empty string: no rundir defined yet (happens iff -W flag was specified, and runlist has not been processed yet)
      - Nonempty string: rundir is defined

    The 'base_filename_list' argument should be a list of json filenames, containing pipeline_objects (not run-lists)
    The 'more_filenames' argument has the same format as 'toplevel_filenames': a list of lists of (suffix, json_filename) pairs
    """

    if len(more_filenames) == 0:
        yield (base_rundir, base_filename_list)
        return

    f0 = more_filenames[0]   # list of (suffix, json_filename) pairs
    f1 = more_filenames[1:]  # j1 = list of lists of (suffix, json_filename) pairs

    for (suffix, f) in f0:
        # d = new base_rundir, formed from base_rundir and suffix.
        if base_rundir is None:
            d = None
        elif len(suffix) == 0:
            d = base_rundir
        elif len(base_rundir) == 0:
            d = suffix
        else:
            d = base_rundir + '_' + suffix

        for t in generate_runs(d, base_filename_list + [f], f1):
            yield t


base_rundir = '' if args.W else args.wv_name
all_runs = list(generate_runs(base_rundir, [ ], toplevel_filenames))

assert len(all_runs) == total_pipeline_runs
assert all((rd is None) or (len(rd) > 0) for (rd,f) in all_runs)

if (total_pipeline_runs > 1) and (not args.n):
    # Check that all rundirs are distinct.  (This catches weird corner cases, for example if
    # one run-list contains suffixes ['a','a_b'], and another run-list contains suffixes ['b_c','c'],
    # then there are two ways to form 'a_b_c'.)

    s = set()

    for (rundir, json_filename_list) in all_runs:
        assert rundir is not None
        if rundir in s:
            raise RuntimeError("rundir '%s' appears more than once in master_run_list" % rundir)
        s.add(rundir)


####################################################################################################
#
# Set up RF_PIPELINE_ATTRS environment variable


extra_attrs = { }

if os.environ.has_key('RF_PIPELINE_ATTRS'):
    extra_attrs = json.loads(os.environ['RF_PIPELINE_ATTRS'])
if os.environ.has_key('RF_PIPELINES_ATTRS'):
    print >>sys.stderr, 'warning: environment variable $RF_PIPELINES_ATTRS was initialized, did you mean to use $RF_PIPELINE_ATTRS?'

if args.nthreads > 1:
    extra_attrs['command_line_parent'] = (' '.join(sys.argv))



####################################################################################################
#
# Run pipeline, case 1: single-run case.


if len(all_runs) == 1:
    (rundir, json_filename_list) = all_runs[0]

    p = [ rf_pipelines.utils.json_read(f) for f in json_filename_list ]
    p = [ rf_pipelines.pipeline_object.from_json(j) for j in p ]
    p = rf_pipelines.pipeline(p) if (len(p) > 1) else p[0]

    extra_attrs2 = copy.copy(extra_attrs)
    extra_attrs2['cwd'] = os.getcwd()
    extra_attrs2['json_files'] = json_filename_list
    extra_attrs2['command_line'] = (' '.join(sys.argv))

    if rundir is not None:
        rf_pipelines.utils.run_for_web_viewer(rundir, p, verbosity=args.verbosity, extra_attrs=extra_attrs2, show_stdout=args.o)
    else:
        rf_pipelines.utils.run_anonymously(p, verbosity=args.verbosity, extra_attrs=extra_attrs, show_stdout=args.o)

    sys.exit(0)


####################################################################################################
#
# Run pipeline, case 2: multi-run case.
#
# Each run happens in its own subprocess.


class asynchronous_pipeline_run:
    def __init__(self, rundir, json_filename_list):
        assert (len(json_filename_list) > 0) and all(f.endswith('.json') for f in json_filename_list)

        # Note that 'rundir' can be None.
        iflag = ['-o'] if args.o else []
        wflag = ['-w', rundir] if (rundir is not None) else ['-n']
        arg_list = ['rfp-run', '-v', str(args.verbosity), '-s' ] + iflag + wflag + json_filename_list

        env = copy.copy(os.environ)
        env['RF_PIPELINE_ATTRS'] = json.dumps(extra_attrs)

        (stdout_rfd, stdout_wfd) = os.pipe()
        (stderr_rfd, stderr_wfd) = os.pipe()

        command_line = ' '.join(arg_list)
        print 'Starting:', command_line

        child_pid = os.fork()

        if child_pid == 0:
            # In child process.
            os.dup2(stdout_wfd, 1)
            os.dup2(stderr_wfd, 2)
            os.close(stdout_rfd)
            os.close(stderr_rfd)
            os.execvpe(arg_list[0], arg_list, env)
            raise RuntimeError('os.execvpe() returned?!')

        # In parent process
        os.close(stdout_wfd)
        os.close(stderr_wfd)

        self.pid = child_pid
        self.command_line = command_line
        self.stdout_fd = stdout_rfd
        self.stderr_fd = stderr_rfd
        self.stdout_str = ''
        self.stderr_str = ''


    @staticmethod
    def _print_indented(label, text):
        if len(text) == 0:
            return

        print '   ', label

        lines = text.split('\n')
        if lines[-1] == '':
            del lines[-1]

        for line in lines:
            print '       ', line


    def _close(self):
        os.close(self.stdout_fd)
        os.close(self.stderr_fd)
        
        self.pid = 0
        self.stdout_fd = -1
        self.stderr_fd = -1


    def read_stdout(self):
        s = os.read(self.stdout_fd, 4096)
        self.stdout_str += s
        return len(s)


    def read_stderr(self):
        s = os.read(self.stderr_fd, 4096)
        self.stderr_str += s
        return len(s)


    def wait(self):
        # self.pid is set to zero when the child process dies.
        assert self.pid > 0

        while self.read_stdout() > 0:
            pass

        while self.read_stderr() > 0:
            pass

        (pid, exit_status) = os.waitpid(self.pid, 0)
        assert pid == self.pid

        self._close()

        status_str = 'Succeeded' if (exit_status == 0) else 'FAILED'
        print '%s: %s' % (status_str, self.command_line)

        self._print_indented('[stdout]', self.stdout_str)
        self._print_indented('[stderr]', self.stderr_str)
        return exit_status
        

    def kill(self):
        if self.pid <= 0:
            return

        try:
            os.kill(self.pid, signal.SIGKILL)
            self._close()
            print 'Killed:', self.command_line
        except:
            pass   # Swallow exception, since kill() only gets called from exception handler anyway.


class thread_pool:
    def __init__(self, run_list, nthreads):
        """The 'run_list' argument should be a list of (rundir, json_filename_list) pairs."""
        
        self.run_list = [ x for x in run_list ]   # shallow copy
        self.nthreads = nthreads
        self.running_threads = [ ]
        self.initial_size = len(run_list)
        self.initial_time = time.time()
        self.num_successful = 0
        self.num_failed = 0


    def __enter__(self):
        return self

        
    def _spawn(self):
        while (len(self.running_threads) < self.nthreads) and (len(self.run_list) > 0):
            (rundir, json_filename_list) = self.run_list[0]
            del self.run_list[0]

            t = asynchronous_pipeline_run(rundir, json_filename_list)
            self.running_threads.append(t)


    def _poll(self):
        if len(self.running_threads) == 0:
            return

        all_fds = set()
        read_fds = set()
        done_fds = set()

        for t in self.running_threads:
            all_fds.add(t.stdout_fd)
            all_fds.add(t.stderr_fd)
            
        p = select.poll()
        for fd in all_fds:
            p.register(fd)

        for (fd, event) in p.poll():
            if event == select.POLLIN:
                read_fds.add(fd)
            elif event == select.POLLHUP:
                done_fds.add(fd)
            else:
                raise RuntimeError('select.poll.poll() returned unrecognized event type')

        unrecognized_fds = read_fds.union(done_fds).difference(all_fds)
        unrecognized_fds = unrecognized_fds.difference(all_fds)

        if len(unrecognized_fds) > 0:
            raise RuntimeError('select.poll.poll() returned unrecognized file descriptor')

        for t in self.running_threads:
            if t.stdout_fd in read_fds:
                t.read_stdout()
            if t.stderr_fd in read_fds:
                t.read_stderr()
            if (t.stdout_fd in done_fds) or (t.stderr_fd in done_fds):
                exit_status = t.wait()
                if exit_status == 0:
                    self.num_successful += 1
                else:
                    self.num_failed += 1

        n = len(self.running_threads)
        self.running_threads = [ t for t in self.running_threads if (t.pid > 0) ]

        threads_exited = len(self.running_threads) < n
        runs_remaining = len(self.run_list) + len(self.running_threads)

        if threads_exited and (runs_remaining > 0):
            plural = '' if (runs_remaining == 1) else 's'
            elapsed_time = time.time() - self.initial_time
            print '    [%d run%s remaining, elapsed time %g seconds]' % (runs_remaining, plural, elapsed_time)

    
    def run(self):
        while (len(self.running_threads) > 0) or (len(self.run_list) > 0):
            self._spawn()
            self._poll()

        print
        print 'Number of successful runs:', self.num_successful
        print 'Number of failed runs:', self.num_failed
        print 'Total elapsed time: %g seconds' % (time.time() - self.initial_time)
    

    def __exit__(self, etype, value, tb):
        for t in self.running_threads:
            t.kill()
        

print 'Total number of pipeline runs:', len(all_runs)

with thread_pool(all_runs, args.nthreads) as p:
    p.run()
