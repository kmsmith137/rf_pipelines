#!/usr/bin/env python
#
# Note: the environment variable RF_PIPELINE_ATTRS can be used to pass additional pipeline attributes.
# These will be passed to the _bind() and _start_pipeline() methods of all pipeline_objects, and are
# also written to the pipeline's json output.


import sys
import argparse

# ArgumentParser subclass, to customize the error message
class MyParser(argparse.ArgumentParser):
    def error(self, message=None):
        print >>sys.stderr, 'Usage: rfp-run.py [-ins] [-w run_name] [-v verbosity] [-t nthreads] file1.json [file2.json file3.json ...]'
        print >>sys.stderr, 'Each json file should contain either a jsonized pipeline_object, or a \"run-list\" of (suffix, pipeline_object_json_filename) pairs'
        print >>sys.stderr, '    -i: interactive mode: stdout/stderr are not redirected to log files'
        print >>sys.stderr, '    -n: runs the pipeline with no output directory (implies -i)'
        print >>sys.stderr, '    -s: throws exception unless single pipeline run (i.e. no run-lists allowed), infrequently used'
        print >>sys.stderr, '    -w: runs the pipeline in a directory which is indexed by the web viewer (frb1 only)'
        print >>sys.stderr, '    -v: specifies pipeline verbosity (integer, default 2)'
        print >>sys.stderr, '    -t: number of threads (default 1, note that multiple threads are only useful if at least one json file is a run-list)'

        if message is not None:
            print >>sys.stderr, '\nError:', message

        sys.exit(2)


####################################################################################################
#
# Argument parsing, checking


parser = MyParser()

parser.add_argument('json_filenames', nargs='*')
parser.add_argument('-i', action='store_true', help='interactive mode: stdout/stderr are not redirected to log files')
parser.add_argument('-n', action='store_true', help='runs the pipeline with no output directory (implies -i)')
parser.add_argument('-s', action='store_true', help='throws exception unless single pipeline run (i.e. no run-lists allowed), infrequently used')
parser.add_argument('-w', dest='wv_name', help='runs the pipeline in a directory which is indexed by the web viewer (frb1 only)')
parser.add_argument('-v', dest='verbosity', type=int, default=2, help='pipeline verbosity (default 2)')
parser.add_argument('-t', dest='nthreads', type=int, default=1, help='number of threads (default 1, note that multiple threads are only useful if at least one json file is a run-list)')

args = parser.parse_args()

ocount = 0
if args.n:
    ocount += 1
if args.wv_name is not None:
    ocount += 1

if len(sys.argv) == 1:
    parser.error()
if ocount != 1:
    parser.error('exactly one of the flags -n,-w must be specified')
if len(args.json_filenames) == 0:
    parser.error('at least one json filename must be specified')
if args.nthreads < 1:
    parser.error('number of threads (i.e. argument to -t option) must be >= 1')
if args.nthreads > 40:
    parser.error('number of threads (i.e. argument to -t option) must be <= 40')

for j in args.json_filenames:
    if not j.endswith('.json'):
        parser.error("filename '%s' does not end in .json, currently treated as an error" % j)

if args.wv_name is not None:
    if args.wv_name.startswith('_') or args.wv_name.endswith('.json'):
        parser.error("invalid web viewer run name '%s" % args.wv_name)
    for s in args.wv_name:
        if s.isspace() or (s == '/'):
            parser.error("invalid web viewer run name '%s" % args.wv_name)


####################################################################################################
#
# Read json files and do some minimal sanity checking.
#
# The result is 'toplevel_filenames', a list of lists of pairs, where
#    outer list index = index of command-line argument
#    inner list index = index within run-list
#    pair = (suffix, json_filename)
#
# If a command-line argument is a pipeline_object (rather than a run-list), then the
# corresponding inner list will have length one, and the 'suffix' part of the pair
# will be an empty string.


import os
import json
import rf_pipelines


def is_well_formed_run_list(j):
    """Helper function: returns True if j is a list of (suffix, json_filename) pairs."""

    if not isinstance(j, list) or (len(j) == 0):
        return False

    for jj in j:
        return isinstance(jj,list) and (len(jj) == 2) and isinstance(jj[0], basestring) and isinstance(jj[1], basestring) and jj[1].endswith('.json')


toplevel_filenames = [ ]

for filename in args.json_filenames:
    j = rf_pipelines.utils.json_read(filename)
    
    if isinstance(j, dict) and j.has_key('class_name'):
        # This appears to be a jsonized pipeline_object.
        # Sanity check: verify that dejsonization works.

        rf_pipelines.pipeline_object.from_json(j)
        toplevel_filenames.append([('',filename)])
        continue

    if not is_well_formed_run_list(j):
        print '%s: not a pipeline_object, or a well-formed run-list consisting of (suffix, json_filename) pairs' % filename
        sys.exit(1)

    inner_filenames = [ ]

    for (suffix, pre_filename2) in j:
        filename2 = os.path.join(os.path.dirname(filename), pre_filename2)

        if (len(suffix) == 0) or (suffix[0] == '_'):
            raise RuntimeError("%s: run-list entry ('%s', '%s') looks suspicious" % (filename, suffix, filename2))
        if not filename2.endswith('.json'):
            raise RuntimeError("%s: run-list entry ('%s', '%s') looks suspicious (second element does not end in '.json')" % (filename, suffix, filename2))
        if args.s:
            raise RuntimeError("%s: run-list cannot be specified with -s" % filename2)

        j2 = rf_pipelines.utils.json_read(filename2)

        if (not isinstance(j2,dict)) or (not j2.has_key('class_name')):
            raise RuntimeError("%s: expected pipeline_object (contained in run-list %s)" % (filename2, filename))

        # Sanity check: verify that dejsonization works.
        rf_pipelines.pipeline_object.from_json(j2)
        inner_filenames.append(('_%s' % suffix, filename2))

    toplevel_filenames.append(inner_filenames)


total_pipeline_runs = 1

for x in toplevel_filenames:
    total_pipeline_runs *= len(x)
    assert total_pipeline_runs > 0

    if total_pipeline_runs > 1000:
        raise RuntimeError('Total number of pipeline runs exceeds 1000, presumably unintentional')

if (total_pipeline_runs > 1) and args.n:
    raise RuntimeError('Run-lists cannot be specified with -n')

if total_pipeline_runs < args.nthreads:
    args.nthreads = total_pipeline_runs


####################################################################################################
#
# Generate 'all_runs', a list of (rundir, json_filename_list) pairs.


def generate_runs(base_rundir, base_filename_list, more_filenames):
    """
    Recursive helper function to generate (rundir, json_filename_list) pairs, where 'rundir' can be None
    (but in this case there is only one (rundir, json_filename_list) pair generated).

    The 'base_rundir' argument can be either a string or None.
    The 'base_filename_list' argument should be a list of json filenames, containing pipeline_objects (not run-lists)
    The 'more_filenames' argument has the same format as 'toplevel_filenames': a list of lists of (suffix, json_filename) pairs
    """

    if len(more_filenames) == 0:
        yield (base_rundir, base_filename_list)
        return

    f0 = more_filenames[0]   # list of (suffix, json_filename) pairs
    f1 = more_filenames[1:]  # j1 = list of lists of (suffix, json_filename) pairs

    if base_rundir is None:
        assert len(f0) == 1

    for (suffix, f) in f0:
        d = (base_rundir + suffix) if (base_rundir is not None) else None
        for t in generate_runs(d, base_filename_list + [f], f1):
            yield t


all_runs = list(generate_runs(args.wv_name, [ ], toplevel_filenames))

assert len(all_runs) == total_pipeline_runs

if len(all_runs) > 1:
    # Check that all rundirs are distinct.  (This catches weird corner cases, for example if
    # one run-list contains suffixes ['a','a_b'], and another run-list contains suffixes ['b_c','c'],
    # then there are two ways to form 'a_b_c'.)

    s = set()

    for (rundir, json_filename_list) in all_runs:
        assert rundir is not None
        if rundir in s:
            raise RuntimeError("rundir '%s' appears more than once in master_run_list" % rundir)
        s.add(rundir)


####################################################################################################
#
# Run pipeline (single-threaded case)


import copy
import time
import traceback

extra_attrs = { }

if os.environ.has_key('RF_PIPELINE_ATTRS'):
    extra_attrs = json.loads(os.environ['RF_PIPELINE_ATTRS'])
if os.environ.has_key('RF_PIPELINES_ATTRS'):
    print >>sys.stderr, 'warning: environment variable $RF_PIPELINES_ATTRS was initialized, did you mean to use $RF_PIPELINE_ATTRS?'

if args.nthreads > 1:
    extra_attrs['command_line_parent'] = (' '.join(sys.argv))


def do_synchronous_pipeline_run(rundir, json_filename_list):
    p = [ rf_pipelines.utils.json_read(f) for f in json_filename_list ]
    p = [ rf_pipelines.pipeline_object.from_json(j) for j in p ]
    p = rf_pipelines.pipeline(p) if (len(p) > 1) else p[0]

    extra_attrs2 = copy.copy(extra_attrs)
    extra_attrs2['cwd'] = os.getcwd()
    extra_attrs2['json_files'] = json_filename_list
    extra_attrs2['command_line'] = (' '.join(sys.argv))

    if rundir is not None:
        rf_pipelines.utils.run_for_web_viewer(rundir, p, verbosity=args.verbosity, extra_attrs=extra_attrs2, redirect_to_log_files = not args.i)
    else:
        assert args.nflag
        print 'Running pipeline with no output directory'
        p.run(outdir=None, verbosity=args.verbosity, extra_attrs=extra_attrs)


# Single-threaded case
if args.nthreads == 1:
    exit_status = 0

    for (rundir, json_filename_list) in all_runs:
        print 'rfp-run: rundir=%s, json_files=%s' % (rundir, json_filename_list)

        try:
            do_synchronous_pipeline_run(rundir, json_filename_list)
        except:
            traceback.print_exc(file=sys.stderr)
            exit_status = 1
            
    sys.exit(exit_status)


####################################################################################################
#
# Run pipeline (multi-threaded case)


import select
import signal
import subprocess


class asynchronous_pipeline_run:
    def __init__(self, rundir, json_filename_list):
        assert rundir is not None
        assert (len(json_filename_list) > 0) and all(f.endswith('.json') for f in json_filename_list)

        arg_list = ['rfp-run', '-w', rundir, '-v', str(args.verbosity), '-s' ] + json_filename_list
        env = copy.copy(os.environ)
        env['RF_PIPELINE_ATTRS'] = json.dumps(extra_attrs)

        (stdout_rfd, stdout_wfd) = os.pipe()
        (stderr_rfd, stderr_wfd) = os.pipe()

        command_line = ' '.join(arg_list)
        print 'Starting:', command_line

        child_pid = os.fork()

        if child_pid == 0:
            # In child process.
            os.dup2(stdout_wfd, 1)
            os.dup2(stderr_wfd, 2)
            os.close(stdout_rfd)
            os.close(stderr_rfd)
            os.execvpe(arg_list[0], arg_list, env)
            raise RuntimeError('os.execvpe() returned?!')

        # In parent process
        os.close(stdout_wfd)
        os.close(stderr_wfd)

        self.pid = child_pid
        self.command_line = command_line
        self.stdout_fd = stdout_rfd
        self.stderr_fd = stderr_rfd
        self.stdout_str = ''
        self.stderr_str = ''


    @staticmethod
    def _print_indented(label, text):
        if len(text) == 0:
            return

        print '   ', label

        lines = text.split('\n')
        if lines[-1] == '':
            del lines[-1]

        for line in lines:
            print '       ', line


    def _close(self):
        os.close(self.stdout_fd)
        os.close(self.stderr_fd)
        
        self.pid = 0
        self.stdout_fd = -1
        self.stderr_fd = -1


    def read_stdout(self):
        s = os.read(self.stdout_fd, 4096)
        self.stdout_str += s
        return len(s)


    def read_stderr(self):
        s = os.read(self.stderr_fd, 4096)
        self.stderr_str += s
        return len(s)


    def wait(self):
        # self.pid is set to zero when the child process dies.
        assert self.pid > 0

        while self.read_stdout() > 0:
            pass

        while self.read_stderr() > 0:
            pass

        (pid, exit_status) = os.waitpid(self.pid, 0)
        assert pid == self.pid

        self._close()

        status_str = 'Succeeded' if (exit_status == 0) else 'FAILED'
        print '%s: %s' % (status_str, self.command_line)

        self._print_indented('[stdout]', self.stdout_str)
        self._print_indented('[stderr]', self.stderr_str)
        return exit_status
        

    def kill(self):
        if self.pid <= 0:
            return

        try:
            os.kill(self.pid, signal.SIGKILL)
            self._close()
            print 'Killed:', self.command_line
        except:
            pass   # Swallow exception, since kill() only gets called from exception handler anyway.


class thread_pool:
    def __init__(self, run_list, nthreads):
        """The 'run_list' argument should be a list of (rundir, json_filename_list) pairs."""
        
        self.run_list = [ x for x in run_list ]   # shallow copy
        self.nthreads = nthreads
        self.running_threads = [ ]
        self.initial_size = len(run_list)
        self.initial_time = time.time()
        self.num_successful = 0
        self.num_failed = 0


    def __enter__(self):
        return self

        
    def _spawn(self):
        while (len(self.running_threads) < self.nthreads) and (len(self.run_list) > 0):
            (rundir, json_filename_list) = self.run_list[0]
            del self.run_list[0]

            t = asynchronous_pipeline_run(rundir, json_filename_list)
            self.running_threads.append(t)


    def _poll(self):
        if len(self.running_threads) == 0:
            return

        all_fds = set()
        read_fds = set()
        done_fds = set()

        for t in self.running_threads:
            all_fds.add(t.stdout_fd)
            all_fds.add(t.stderr_fd)
            
        p = select.poll()
        for fd in all_fds:
            p.register(fd)

        for (fd, event) in p.poll():
            if event == select.POLLIN:
                read_fds.add(fd)
            elif event == select.POLLHUP:
                done_fds.add(fd)
            else:
                raise RuntimeError('select.poll.poll() returned unrecognized event type')

        unrecognized_fds = read_fds.union(done_fds).difference(all_fds)
        unrecognized_fds = unrecognized_fds.difference(all_fds)

        if len(unrecognized_fds) > 0:
            raise RuntimeError('select.poll.poll() returned unrecognized file descriptor')

        for t in self.running_threads:
            if t.stdout_fd in read_fds:
                t.read_stdout()
            if t.stderr_fd in read_fds:
                t.read_stderr()
            if (t.stdout_fd in done_fds) or (t.stderr_fd in done_fds):
                exit_status = t.wait()
                if exit_status == 0:
                    self.num_successful += 1
                else:
                    self.num_failed += 1

        n = len(self.running_threads)
        self.running_threads = [ t for t in self.running_threads if (t.pid > 0) ]

        threads_exited = len(self.running_threads) < n
        runs_remaining = len(self.run_list) + len(self.running_threads)

        if threads_exited and (runs_remaining > 0):
            plural = '' if (runs_remaining == 1) else 's'
            elapsed_time = time.time() - self.initial_time
            print '    [%d run%s remaining, elapsed time %g seconds]' % (runs_remaining, plural, elapsed_time)

    
    def run(self):
        while (len(self.running_threads) > 0) or (len(self.run_list) > 0):
            self._spawn()
            self._poll()

        print
        print 'Number of successful runs:', self.num_successful
        print 'Number of failed runs:', self.num_failed
        print 'Total elapsed time: %g seconds' % (time.time() - self.initial_time)
    

    def __exit__(self, etype, value, tb):
        for t in self.running_threads:
            t.kill()
        

with thread_pool(all_runs, args.nthreads) as p:
    p.run()
